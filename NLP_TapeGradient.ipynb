{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "class CustomBERTModel(keras.Model):\n",
    "    def __init__(self):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.bert = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "          ### New layers:\n",
    "          self.linear1 = keras.layers.Dense(256)\n",
    "          self.linear2 = keras.layers.Dense(2) ## 2 is the number of classes in this example\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "          # call expects only one positional argument, so you have to pass in a tuple and unpack. The next parameter is a special reserved training parameter.\n",
    "          ids, mask = inputs\n",
    "          sequence_output = self.bert(ids, mask, training=training).last_hidden_state\n",
    "\n",
    "          # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "          linear1_output = self.linear1(sequence_output[:,0,:]) ## extract the 1st token's embeddings\n",
    "\n",
    "          linear2_output = self.linear2(linear1_output)\n",
    "\n",
    "          return linear2_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomBERTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_metric.reset_states()\n",
    "val_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, tf_train_dataset, tf_test_dataset, epochs=2):\n",
    "  # train the model by using GradientTape\n",
    "  optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "  loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "  train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "  val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "  for epoch in range(epochs):\n",
    "      print(f\"\\nStart of Training Epoch {epoch}\")\n",
    "      for step, batch in enumerate(tf_train_dataset):\n",
    "          # print(step)\n",
    "          # print(batch)\n",
    "          ids = batch[0]['input_ids']\n",
    "          mask = batch[0]['attention_mask']\n",
    "          y = batch[1]\n",
    "          with tf.GradientTape() as tape:\n",
    "              logits = model((ids, mask), training=True)\n",
    "              loss_value = loss_fn(y, logits)\n",
    "              # print(f\"Loss at step {step}: {loss_value}\")\n",
    "          grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "          # Filter trainable weights that have gradients\n",
    "          trainable_vars = [var for var, grad in zip(model.trainable_weights, grads) if grad is not None]\n",
    "\n",
    "          # optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "          optimizer.apply_gradients(\n",
    "                        (grad, var)\n",
    "                        for (grad, var) in zip(grads, model.trainable_variables)\n",
    "                        if grad is not None\n",
    "                      )\n",
    "          # Update training metric.\n",
    "          train_acc_metric(y, logits)\n",
    "\n",
    "          # Log every 200 batches.\n",
    "          if step % 10 == 0:\n",
    "              print(\n",
    "                  \"Training loss at step %d: %.4f\"\n",
    "                  % (step, float(loss_value))\n",
    "              )\n",
    "              #print accuracy on the training set\n",
    "              train_acc = train_acc_metric.result()\n",
    "              print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "          # Display metrics at the end of each epoch.\n",
    "\n",
    "      train_acc_metric.reset_states()\n",
    "      # perform validation on test data\n",
    "      for step, batch in enumerate(tf_test_dataset):\n",
    "          ids = batch[0]['input_ids']\n",
    "          mask = batch[0]['attention_mask']\n",
    "          y = batch[1]\n",
    "          logits = model([ids, mask], training=False)\n",
    "          # Update val metrics\n",
    "          val_acc_metric(y, logits)\n",
    "      val_acc = val_acc_metric.result()\n",
    "      # print accuracy on the test set\n",
    "      print(\"Test acc: %.4f\" % (float(val_acc),))\n",
    "      # Reset val metrics at the end of each epoch\n",
    "      val_acc_metric.reset_states()\n",
    "\n",
    "  model.save_weights('my_model', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(new_model, tf_train_dataset, tf_test_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The recommended way to save a subclassed model is to use save_weights to create a TensorFlow SavedModel checkpoint\n",
    "model.save_weights('tape_model', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore the loaded model\n",
    "new_model = CustomBERTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[train_acc_metric])\n",
    "# call the model on part of the training set to build the model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
