{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Datasets\n",
    "!pip install transformers\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import datasets\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTModel(keras.Model):\n",
    "    def __init__(self):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.bert = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "          ### New layers:\n",
    "          self.linear1 = keras.layers.Dense(256)\n",
    "          self.linear2 = keras.layers.Dense(2) ## 2 is the number of classes in this example\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "          # call expects only one positional argument, so you have to pass in a tuple and unpack. The next parameter is a special reserved training parameter.\n",
    "          ids, mask = inputs\n",
    "          sequence_output = self.bert(ids, mask, training=training).last_hidden_state\n",
    "\n",
    "          # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "          linear1_output = self.linear1(sequence_output[:,0,:]) ## extract the 1st token's embeddings\n",
    "\n",
    "          linear2_output = self.linear2(linear1_output)\n",
    "\n",
    "          return linear2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use for debug only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, max_length=128):\n",
    "    return tokenizer(examples[\"text\"],\n",
    "                     truncation=True,\n",
    "                     padding='max_length',\n",
    "                     max_length=max_length,\n",
    "                     return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_path):\n",
    "   train_df = pd.read_json(train_path, lines=True)\n",
    "   train_df = train_df[['text', 'label']]\n",
    "   train_df, test_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=42)\n",
    "   return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/subtaskA_train_monolingual.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = get_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only 100 sample from training data (test purposes)\n",
    "train_df = train_df[:100]\n",
    "test_df = test_df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"human\", 1: \"machine\"}\n",
    "label2id = {\"human\": 0, \"machine\": 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataframe to huggingface Dataset\n",
    "train_dataset= Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.DatasetDict({'train': train_dataset, 'test': test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello, this is a sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_tokenizer_columns = set(dataset[\"train\"].features)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "# tokenizer_columns = list(set(encoded_dataset[\"train\"].features) - pre_tokenizer_columns)\n",
    "# print(\"Columns added by tokenizer:\", tokenizer_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset[\"train\"].features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomBERTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label2id), id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = prepare_model.prepare_tf_dataset(encoded_dataset[\"train\"],\n",
    "                                      batch_size=8,\n",
    "                                      shuffle=True,\n",
    "                                      tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_dataset = prepare_model.prepare_tf_dataset(encoded_dataset[\"train\"],\n",
    "                                          batch_size=8,\n",
    "                                          shuffle=False,\n",
    "                                          tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del prepare_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[train_acc_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, tf_train_dataset, tf_test_dataset, epochs=2):\n",
    "  # train the model by using GradientTape\n",
    "  optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "  \n",
    "  for epoch in range(epochs):\n",
    "      print(f\"\\nStart of Training Epoch {epoch}\")\n",
    "      for step, batch in enumerate(tf_train_dataset):\n",
    "          ids = batch[0]['input_ids']\n",
    "          mask = batch[0]['attention_mask']\n",
    "          y = batch[1]\n",
    "\n",
    "          with tf.GradientTape() as tape:\n",
    "              logits = model((ids, mask), training=True)\n",
    "              loss_value = loss_fn(y, logits)\n",
    "\n",
    "          grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "          \n",
    "          optimizer.apply_gradients(\n",
    "                        (grad, var)\n",
    "                        for (grad, var) in zip(grads, model.trainable_variables)\n",
    "                        if grad is not None\n",
    "                      )\n",
    "          # Update training metric.\n",
    "          train_acc_metric(y, logits)\n",
    "\n",
    "          # Log every 200 batches.\n",
    "          if step % 10 == 0:\n",
    "              print(\n",
    "                  \"Training loss at step %d: %.4f\"\n",
    "                  % (step, float(loss_value))\n",
    "              )\n",
    "              #print accuracy on the training set\n",
    "              train_acc = train_acc_metric.result()\n",
    "              print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "          # Display metrics at the end of each epoch.\n",
    "\n",
    "      train_acc_metric.reset_states()\n",
    "\n",
    "      # perform validation on test data\n",
    "      for step, batch in enumerate(tf_test_dataset):\n",
    "          ids = batch[0]['input_ids']\n",
    "          mask = batch[0]['attention_mask']\n",
    "          y = batch[1]\n",
    "          logits = model([ids, mask], training=False)\n",
    "          # Update val metrics\n",
    "          val_acc_metric(y, logits)\n",
    "\n",
    "      val_acc = val_acc_metric.result()\n",
    "\n",
    "      # print accuracy on the test set\n",
    "      print(\"Test acc: %.4f\" % (float(val_acc),))\n",
    "\n",
    "      # Reset val metrics at the end of each epoch\n",
    "      val_acc_metric.reset_states()\n",
    "\n",
    "  model.save_weights('my_model', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(model, tf_train_dataset, tf_test_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The recommended way to save a subclassed model is to use save_weights to create a TensorFlow SavedModel checkpoint\n",
    "model.save_weights('my_model', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CustomBERTModel()\n",
    "loaded_model.load_weights('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the loaded model on a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a batch from the training dataset\n",
    "batch = next(iter(tf_train_dataset))\n",
    "\n",
    "# Separate input data and target data from the batch\n",
    "ids = batch[0]['input_ids']\n",
    "mask = batch[0]['attention_mask']\n",
    "y = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model on the extracted batch\n",
    "loss_value = loaded_model.train_on_batch((ids, mask), y)\n",
    "print(f\"Loss: {loss_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the model has been preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_metric.reset_state()\n",
    "for step, batch in enumerate(tf_test_dataset):\n",
    "        ids = batch[0]['input_ids']\n",
    "        mask = batch[0]['attention_mask']\n",
    "        y = batch[1]\n",
    "        logits = loaded_model([ids, mask], training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric(y, logits)\n",
    "val_acc = val_acc_metric.result()\n",
    "# print accuracy on the test set\n",
    "print(\"test acc: %.4f\" % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the model state has been preserved\n",
    "new_predictions = loaded_model.predict(tf_test_dataset)\n",
    "# np.testing.assert_allclose(predictions, new_predictions, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the custom model to file (not suported for custom class)\n",
    "model.save('path_to_my_model',save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "new_model = keras.models.load_model('path_to_my_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
